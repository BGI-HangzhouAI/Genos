{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e90b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0520d81",
   "metadata": {},
   "source": [
    "### 设置随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0af5b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置random seed保证可重复性\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# For numpy\n",
    "np.random.seed(seed)\n",
    "# For deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff7522",
   "metadata": {},
   "source": [
    "### 提取和存embedding\n",
    "##### 此处没有对序列做truncation，请确保输入序列长度 ≤ model_max_length (model_max_length = tokenizer.model_max_length)\n",
    "#### 提取embedding时不涉及到padding，batch内所有序列长度相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68fac544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONLDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        self.data = []\n",
    "\n",
    "        # 读取 JSONL 文件\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:  # 跳过空行\n",
    "                    item = json.loads(line)\n",
    "                    self.data.append(item)\n",
    "\n",
    "        print(f\"[INFO] Loaded {len(self.data)} samples from {file_path}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08448fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_jsonl(dataset_name):\n",
    "    train_dataset = JSONLDataset(f\"./data/{dataset_name}_train.jsonl\")\n",
    "    eval_dataset = JSONLDataset(f\"./data/{dataset_name}_eval.jsonl\")\n",
    "    test_dataset = JSONLDataset(f\"./data/{dataset_name}_test.jsonl\")\n",
    "        \n",
    "    return train_dataset, eval_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18bda2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer):\n",
    "    # labels of cage is saved in a string of list\n",
    "    labels = [int(item[\"label\"]) for item in batch]\n",
    "    labels = torch.tensor(labels)\n",
    "    sequences = [item[\"seq\"] for item in batch] \n",
    "    encoding = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    return encoding, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4618e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 提取 embedding（mean pooling last_hidden_state）\n",
    "# -------------------------------\n",
    "def calc_embeddings(hf_inputs, model, device):\n",
    "    hf_inputs = {k: v.to(device) for k, v in hf_inputs.items()}\n",
    "    mask = hf_inputs.get(\n",
    "        \"attention_mask\", \n",
    "        torch.ones_like(hf_inputs['input_ids'])\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**hf_inputs, output_hidden_states=True)\n",
    "        last_hidden = outputs.hidden_states[-1]  # [B, T, H]\n",
    "        mask = mask.unsqueeze(-1)  # [B, T, 1]\n",
    "        pooled = (last_hidden * mask).sum(1) / mask.sum(1) #只对有效token做mean pooling [B, H]\n",
    "        pooled = pooled.float()  # [B, H]\n",
    "    return pooled\n",
    "\n",
    "def extract_embeddings(model, tokenizer, dataloader, device):\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    for batch in tqdm(dataloader, desc=\"Embedding\"):\n",
    "        inputs, labels = batch\n",
    "        pooled = calc_embeddings(inputs, model, device)\n",
    "        all_embeddings.append(pooled.detach().cpu())\n",
    "        all_labels.append(labels)\n",
    "    return torch.cat(all_embeddings), torch.cat(all_labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29c8ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 存embedding\n",
    "# -------------------------------\n",
    "\n",
    "def _process_embeddings_and_save(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    data_save_path,\n",
    "    device,\n",
    "): \n",
    "    data_loader = DataLoader(\n",
    "        dataset, batch_size=1, shuffle=False,\n",
    "        collate_fn=lambda x: collate_fn(x, tokenizer),\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    embedding_data = extract_embeddings(model, tokenizer, data_loader, device)\n",
    "    X, y = embedding_data\n",
    "    torch.save({\n",
    "        \"embeddings\": X.detach().cpu(),\n",
    "        \"labels\": y.detach().cpu(),\n",
    "    }, data_save_path)\n",
    "\n",
    "def extract_embeddings_on_dataset(model, tokenizer, dataset_name, device, embedding_dir, model_name):\n",
    "    train_dataset, eval_dataset, test_dataset = load_dataset_jsonl(dataset_name)\n",
    "    \n",
    "    #columns = test_dataset.column_names\n",
    "    #print(columns)\n",
    "    \n",
    "    if not os.path.exists(embedding_dir):\n",
    "        os.makedirs(f\"{embedding_dir}\")\n",
    "        \n",
    "    data_train_path = f\"{embedding_dir}/{dataset_name}_train.pt\" if os.path.exists(embedding_dir) else f\"./{dataset_name}_train.pt\"\n",
    "    _process_embeddings_and_save(model, tokenizer, train_dataset, data_train_path, device)\n",
    "    \n",
    "    data_eval_path = f\"{embedding_dir}/{dataset_name}_eval.pt\" if os.path.exists(embedding_dir) else f\"./{dataset_name}_eval.pt\"\n",
    "    _process_embeddings_and_save(model, tokenizer, eval_dataset, data_eval_path, device)\n",
    "    \n",
    "    data_test_path = f\"{embedding_dir}/{dataset_name}_test.pt\" if os.path.exists(embedding_dir) else f\"./{dataset_name}_test.pt\"\n",
    "    _process_embeddings_and_save(model, tokenizer, test_dataset, data_test_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffd9bac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model from /data/model/Genos-1.2B\n",
      "[INFO] Loaded 150 samples from ./data/Human_classify_8k_train.jsonl\n",
      "[INFO] Loaded 150 samples from ./data/Human_classify_8k_eval.jsonl\n",
      "[INFO] Loaded 150 samples from ./data/Human_classify_8k_test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 150/150 [00:12<00:00, 12.19it/s]\n",
      "Embedding: 100%|██████████| 150/150 [00:11<00:00, 12.61it/s]\n",
      "Embedding: 100%|██████████| 150/150 [00:11<00:00, 12.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# embedding 提取主流程\n",
    "# -------------------------------\n",
    "model_name = \"Genos-1.2B\"\n",
    "model_path = \"/data/model/Genos-1.2B\"\n",
    "datasets = [\"Human_classify_8k\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"[INFO] Loading model from {model_path}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_path,\n",
    "    # 如果环境中不支持，可以注释掉下面这行\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "embedding_dir = f\"./embedding/{model_name}/\"\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    extract_embeddings_on_dataset(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        dataset_name,\n",
    "        device,\n",
    "        embedding_dir,\n",
    "        model_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da932e07",
   "metadata": {},
   "source": [
    "### 加载embedding，并训练XGboost分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ba686ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_classifier(X_train, y_train, X_test, random_state=42):\n",
    "    \"\"\"训练XGBoost分类器\"\"\"\n",
    "    X_train_np = X_train.cpu().numpy() if torch.is_tensor(X_train) else X_train\n",
    "    y_train_np = y_train.cpu().numpy() if torch.is_tensor(y_train) else y_train\n",
    "    X_test_np = X_test.cpu().numpy() if torch.is_tensor(X_test) else X_test\n",
    "    \n",
    "    print(\"Training XGBoost classifier...\")\n",
    "    print(\"XGboost parameters: n_estimators=100, learning_rate=0.1, max_depth=6, random_state={}\".format(random_state))\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=random_state,\n",
    "        # use_label_encoder=False, \n",
    "        eval_metric='mlogloss',\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6        \n",
    "    )\n",
    "    xgb.fit(X_train_np, y_train_np)\n",
    "    print(\"XGBoost training completed\")\n",
    "    \n",
    "    probs = xgb.predict_proba(X_test_np)\n",
    "    preds = xgb.predict(X_test_np)\n",
    "    return preds, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69261e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_dataset(dataset_name, device, embedding_dir, method='xgboost'):\n",
    "    \n",
    "    data_train_path = f\"{embedding_dir}/{dataset_name}_train.pt\"\n",
    "    data_eval_path = f\"{embedding_dir}/{dataset_name}_eval.pt\"\n",
    "    data_test_path = f\"{embedding_dir}/{dataset_name}_test.pt\"\n",
    "\n",
    "    print(f\"[INFO] Loading train data from {data_train_path}\")\n",
    "    train_data = torch.load(data_train_path)\n",
    "    X_train = train_data[\"embeddings\"]\n",
    "    y_train = train_data[\"labels\"]\n",
    "\n",
    "    print(f\"[INFO] Loading validation data from {data_eval_path}\")\n",
    "    eval_data = torch.load(data_eval_path)\n",
    "    X_val = eval_data[\"embeddings\"]\n",
    "    y_val = eval_data[\"labels\"]\n",
    "\n",
    "    print(f\"[INFO] Loading test data from {data_test_path}\")\n",
    "    test_data = torch.load(data_test_path)\n",
    "    X_test = test_data[\"embeddings\"]\n",
    "    y_test = test_data[\"labels\"]\n",
    "\n",
    "    print(f\"\\n[INFO] Label distribution:\")\n",
    "    train_counts = np.bincount(y_train.numpy())\n",
    "    val_counts = np.bincount(y_val.numpy())\n",
    "    test_counts = np.bincount(y_test.numpy())\n",
    "    \n",
    "    print(f\"Train set: {train_counts} (total: {len(y_train)})\")\n",
    "    print(f\"Validation set: {val_counts} (total: {len(y_val)})\")\n",
    "    print(f\"Test set: {test_counts} (total: {len(y_test)})\")\n",
    "    \n",
    "    # 因为Xgboost不使用验证集，因此仅使用原始训练集和测试集\n",
    "    print(f\"\\n[INFO] Since XGboost does not use validation set, using train and test sets without validation set.\")\n",
    "    print(f\"Train set size: {len(X_train)}\")\n",
    "    print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "    # 使用训练集训练，在处理后的测试集上评估\n",
    "    y_pred, y_probs = train_xgboost_classifier(\n",
    "            X_train, y_train, X_test,\n",
    "            random_state=42\n",
    "        )\n",
    "    y_true = y_test.numpy()\n",
    "\n",
    "\n",
    "    # 计算评估指标\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    try:\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    except:\n",
    "        mcc = 0.0\n",
    "\n",
    "    try:\n",
    "        if y_probs.shape[1] == 2:\n",
    "            auc = roc_auc_score(y_true, y_probs[:, 1])\n",
    "            auprc = average_precision_score(y_true, y_probs[:, 1])\n",
    "        else:\n",
    "            auc = roc_auc_score(y_true, y_probs, multi_class='ovr', average='macro')\n",
    "            auprc = average_precision_score(y_true, y_probs, average='macro')\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating AUC/AUPRC: {e}\")\n",
    "        auc = 0.0\n",
    "        auprc = 0.0\n",
    "\n",
    "    # 打印每个类别的正确率\n",
    "    print(f\"\\n[INFO] Per-class accuracy:\")\n",
    "    for class_idx in range(4):\n",
    "        class_mask = (y_true == class_idx)\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_correct = np.sum((y_pred[class_mask] == class_idx))\n",
    "            class_accuracy = class_correct / np.sum(class_mask)\n",
    "            print(f\"  Class {class_idx}: {class_accuracy:.4f} ({class_correct}/{np.sum(class_mask)})\")\n",
    "        else:\n",
    "            print(f\"  Class {class_idx}: No samples in test set\")\n",
    "\n",
    "    # 打印错误预测分布\n",
    "    print(f\"\\n[INFO] Error prediction distribution:\")\n",
    "    for true_class in range(4):\n",
    "        true_class_mask = (y_true == true_class)\n",
    "        true_class_indices = np.where(true_class_mask)[0]\n",
    "\n",
    "        if len(true_class_indices) > 0:\n",
    "            preds_for_true_class = y_pred[true_class_mask]\n",
    "            wrong_pred_mask = (preds_for_true_class != true_class)\n",
    "            wrong_preds = preds_for_true_class[wrong_pred_mask]\n",
    "\n",
    "            if len(wrong_preds) > 0:\n",
    "                error_counts = np.bincount(wrong_preds, minlength=4)\n",
    "                total_errors = len(wrong_preds)\n",
    "                print(f\"  For true class {true_class} (errors: {total_errors}/{len(true_class_indices)}):\")\n",
    "                for pred_class in range(4):\n",
    "                    if pred_class != true_class and error_counts[pred_class] > 0:\n",
    "                        percentage = (error_counts[pred_class] / total_errors) * 100\n",
    "                        print(f\"    → Predicted as class {pred_class}: {error_counts[pred_class]} ({percentage:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"  For true class {true_class}: No prediction errors\")\n",
    "        else:\n",
    "            print(f\"  For true class {true_class}: No samples in test set\")\n",
    "\n",
    "    return acc, auc, auprc, f1, mcc, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad7aff5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading train data from ./embedding/Genos-1.2B//Human_classify_8k_train.pt\n",
      "[INFO] Loading validation data from ./embedding/Genos-1.2B//Human_classify_8k_eval.pt\n",
      "[INFO] Loading test data from ./embedding/Genos-1.2B//Human_classify_8k_test.pt\n",
      "\n",
      "[INFO] Label distribution:\n",
      "Train set: [50 50 50] (total: 150)\n",
      "Validation set: [50 50 50] (total: 150)\n",
      "Test set: [50 50 50] (total: 150)\n",
      "\n",
      "[INFO] Since XGboost does not use validation set, using train and test sets without validation set.\n",
      "Train set size: 150\n",
      "Test set size: 150\n",
      "Training XGBoost classifier...\n",
      "XGboost parameters: n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42\n",
      "XGBoost training completed\n",
      "\n",
      "[INFO] Per-class accuracy:\n",
      "  Class 0: 0.1800 (9/50)\n",
      "  Class 1: 0.2800 (14/50)\n",
      "  Class 2: 0.4200 (21/50)\n",
      "  Class 3: No samples in test set\n",
      "\n",
      "[INFO] Error prediction distribution:\n",
      "  For true class 0 (errors: 41/50):\n",
      "    → Predicted as class 1: 20 (48.8%)\n",
      "    → Predicted as class 2: 21 (51.2%)\n",
      "  For true class 1 (errors: 36/50):\n",
      "    → Predicted as class 0: 18 (50.0%)\n",
      "    → Predicted as class 2: 18 (50.0%)\n",
      "  For true class 2 (errors: 29/50):\n",
      "    → Predicted as class 0: 15 (51.7%)\n",
      "    → Predicted as class 1: 14 (48.3%)\n",
      "  For true class 3: No samples in test set\n",
      "\n",
      "Completed Human_classify_8k task with XGboost: Acc=0.2933, AUC=0.5065, F1=0.2877\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# XGboost 分类主流程\n",
    "# -------------------------------\n",
    "datasets = [\"Human_classify_8k\"]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    acc, auc, auprc, f1, mcc, precision, recall = evaluate_model_on_dataset(dataset, \n",
    "                                                device, embedding_dir, method='xgboost')\n",
    "print(f\"\\nCompleted {dataset} task with XGboost: Acc={acc:.4f}, AUC={auc:.4f}, F1={f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad2145f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
