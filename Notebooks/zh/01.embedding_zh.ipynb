{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bfa7242",
   "metadata": {},
   "source": [
    "# 生物序列Embedding提取教程\n",
    "\n",
    "## 什么是Embedding？\n",
    "\n",
    "**Embedding（嵌入向量）** 是将文本、序列或其他非结构化数据转换为数值向量的技术。在生物信息学中，embedding能够将DNA序列、蛋白质序列等生物序列转换为高维数值向量，这些向量能够：\n",
    "\n",
    "1. **捕获序列的语义信息**：相似的序列会产生相似的向量\n",
    "2. **支持机器学习**：数值向量可以直接用于各种机器学习算法\n",
    "3. **降维表示**：将复杂的序列信息压缩为固定长度的向量\n",
    "4. **计算相似性**：通过向量距离计算序列间的相似性\n",
    "\n",
    "## 为什么要提取Embedding？\n",
    "\n",
    "在生物信息学研究中，embedding提取具有重要价值：\n",
    "\n",
    "- **序列分类**：识别DNA序列的功能类型（如启动子、增强子等）\n",
    "- **序列相似性分析**：快速找到相似的生物序列\n",
    "- **功能预测**：基于序列embedding预测蛋白质功能\n",
    "- **进化分析**：研究序列的进化关系\n",
    "\n",
    "## 本教程将演示：\n",
    "\n",
    "1. 如何加载预训练的生物序列模型\n",
    "2. 如何将DNA序列转换为embedding向量\n",
    "3. 如何分析不同层的embedding特征\n",
    "4. 理解embedding向量的含义和应用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd18121",
   "metadata": {},
   "source": [
    "## 步骤1：导入必要的库\n",
    "\n",
    "首先，我们需要导入处理embedding所需的Python库：\n",
    "\n",
    "- **torch**：PyTorch深度学习框架，用于模型推理\n",
    "- **transformers**：Hugging Face的transformers库，提供预训练模型和分词器\n",
    "  - **AutoModel**：自动加载预训练模型\n",
    "  - **AutoTokenizer**：自动加载对应的分词器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf26eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556a90c",
   "metadata": {},
   "source": [
    "## 步骤2：加载预训练模型\n",
    "\n",
    "这里我们加载一个专门用于生物序列的预训练模型：\n",
    "\n",
    "- **model_path**：指定预训练模型的路径（这里使用的是生物序列专用模型）\n",
    "- **tokenizer**：分词器，负责将DNA序列转换为模型可以理解的token\n",
    "- **model**：预训练模型，设置`output_hidden_states=True`以获取所有层的隐藏状态\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5658060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MixtralModel(\n",
       "  (embed_tokens): Embedding(128, 1024, padding_idx=14)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x MixtralDecoderLayer(\n",
       "      (self_attn): MixtralAttention(\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "        (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "        (gate): Linear(in_features=1024, out_features=8, bias=False)\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
       "            (w1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "            (w2): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (w3): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (input_layernorm): MixtralRMSNorm((1024,), eps=1e-05)\n",
       "      (post_attention_layernorm): MixtralRMSNorm((1024,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (norm): MixtralRMSNorm((1024,), eps=1e-05)\n",
       "  (rotary_emb): MixtralRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定模型路径\n",
    "model_path = \"/path/to/your/local/Genos-1.2B\" # 替换为本地模型路径\n",
    "\n",
    "# 加载分词器和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 加载本地下载好的Genos的权重\n",
    "model = AutoModel.from_pretrained(model_path, \n",
    "                                  output_hidden_states=True,\n",
    "                                  torch_dtype=torch.bfloat16, \n",
    "                                  trust_remote_code=True, \n",
    "                                  attn_implementation=\"flash_attention_2\" # 使用flash_attention\n",
    "  )\n",
    "model.cuda() # 将模型加载到GPU\n",
    "model.eval() # 将模型切换到推理模式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78af7f3",
   "metadata": {},
   "source": [
    "## 步骤3：准备输入序列\n",
    "\n",
    "我们创建一个DNA序列作为示例：\n",
    "\n",
    "- **text**：随机生成指定长度的序列\n",
    "- 这个序列包含了常见的DNA碱基（A、T、G、C）\n",
    "\n",
    "**实际应用中**，你可以替换为任何真实的DNA序列。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5f1b839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCTACGTGGTTCACATAGCCGTACCACCGTTCGGGCCATAGCGGCCGTGTATTGAGAGAATGGCCTTCATATCCGGTTTGGCCAGGATTATACTAGTTGAAGCCAGCGGCCTTCGTAGGGCGACGACTTATGTACCCATACGATGCTGATAGCTCAGGGTGACGCAATGCATAGACCCTTTGACAGTCAAATGTGCTGTTGAGGTACGACTGCCATTCGGTTTGATAGACAGTCCGCGTTGGCTGTTACATTGTATAGTGCGATACTATATATGCGTAATGAATCGGTACCGTTGTTATATGCTTATAATTGCTAGACGTGTGCCATATTTTGTGCGAAACAGGCCCGCGAATTGAATGCGTCCCCAAGACAAGGAAATAGCAATAATGGGGATGCTCCCGAAGATCCTGCAGTTTATCGTTAAATAAACTTCGTGATGTCAATACCAGCCTCGTATTTCATAAGAGAGCTCCAGACAGTTCATTCGGCGACACGGCGGAAGCAGTAGCCGCCGTATTACTTATCGAATGTTCCTATCAAGTGGGACCCGTTGCCGACTGCGGCTTGCGTGAAATAGACTTTTACCCAAAGCAACTCATAGCGAAGGCAGTGTCCACTCGCCCGTAACATCCCACCATGTTATCGTCGATGGTAGCAGCTATCCCAACGCGCCTACTCCCGCGAAAGACCCGAGAACTGGCGAGACGTGTTTGCCCGGACAGAACGCTGGAAAAATTCAGTGTATCTATAGTAGGCGCGTTACATGTCGACGCAAGCTTTTCATGAGTGAGTGCCGACATCAAGAATATTTAGATAATATGAGATCGCTCCAGCAATGGCATGAATGCTGGATCGTTCCGGCTCTCCACCAATGTCTTAGCCCCGCGTGGTCTCTCGAGGGACTGTAGATTGGACACTCCGTCTCTACATACGGTTTGGGACATTTGTTGGATCTGGAGGGAGCCCTTAAGGACAATCCAGGAAACCTTTGGCAGTCCCTAAGTGATAATGGATACACATTTGGAAGAATACGGTCGTTAAGGAAAATATTTCCGTCAGTCGAGTCTTAACGCAGTTGTCGTACACTGTCTAGGACCGAGCCGCGAAGAGAACGAACGTTGGGACCTATTGCCCGCTTCCGATTTAAAAGCCCCGATTCTACGCACAAGTGGCACCAATTCGGATTATCTAGAGGATTTCTGTTGATCCACACACCCTTCTGGCGAAATAATGACAGGGACCGAGCGGTACAAAGTATTGCCTCAGGGCGTACCGTAGCGATTCCAAGGGCCTTGTAACCTCGGAAATAGCCTGACTAACGGGCCAGCTTCTTGAAGCCCGCGTAGCATGCTCGTATCCTCAGTTCATGATGTGAGAATCGTACGATGCTTATGGGTCTGTAAGTGCATTTATCCCCAAGATGGACCGAAGTAGTATAACCATTAGGCAACACACTACGTGCGTGACCCATAAGGGGTCCCTAGCGCTTGCTCTAAACGAGCATGACTCGCCAACCATATTAGTAGTGTCTCTGACAAGCGCGCACACGTTGGCGGTTGCCGTCAACGCGTGTGGAAGGTATGATAGCTTTTCCGTATCGAGTCGGCCATATAAGGTTGAACAAAGACTATATTGTTATTTTTTGAGGATCCTCTAAGGGCCCTCACCATCTCTGCAATGGGCTAAAGGCCGTTTGACGGACGATAACCTGGACCTTCACAACCTTTCTTGAGAAAAGGTCCTAAGTCCCGCACGCCGTGCCCGAGCTTAACGATGCCGTAAAGCTATTTAGCGCCATCGTTCTTTTTCGTTGACGATCAGTACGCTAGCTGAGCTAACTGTTAGTTGCGGCATTCATGAGAATACTGACGTGAACTATGAAATCGAAAACAATGGCCTTCGTCATAAGGTTGCCCATTCCCGAGCTTCAACGCACTTCCATTGATGGCCCGTAAGGCCCTGGTCTAATAAATAGGAACCCCCTTTCGCGTTTTGAGATAAGAGTATGGTCAATGCTCTTAAGCTAGTACATTAAAACTCTTCACCGGCCATTGAGGTGAAACAGCCGCAAATGGTTGCACTATCGCTTTCAGCCCGCTGCCTCTACTCGGAGGCTCGAGCATGTCGTCCTTGGCTCACGTAAAGGCCAGGAAGTCCAGAAGTCGTTACGCCGGCTGTAAACCCGCATAGGGGGGCGGTCACGATCGCGCTGAACGTATTAAGTGCCTACTCTGCGCCTGCCTATCATATCGGCACCGAAACTTCTGTGAAGAACGATGTTTCTTTATAAAGCGCTTCTTCTGCTCTATCCCTGCGGTACTCTGATACGATCCTCAGTTACAAAGTGACGGCTTTTACGAATTCACCTAACCGTCACTAAGAGCTGCAGGACCTCCGACTCTCACCCTCGCTAACCGCAATATCCGCCTTACCACTAGTGGGAGATCAATAGACTCTGGAGGTCTTTGTGGCTATATCTCAGTCATCACGTCTGTGCACTATCCGTTTACCGCCGAAACGGAAGATACGCATGCGAGTGTCGTGCCCGAGAACCCTTACCCAAGAATCAACGTATCACCGGCTCTCTCTCGAGCGTCCATTACGCAAACTCTATGTATACAATTATCATTTCAAGAGGGCTTTTCCGATCTCGTGTTAAGACGTGGGAGGAATCACCGCCGCGTGGTCTCTGCCGTAACTGGAGATTTGTCTTTCTTGCCGTAGTTCCTTCAGCTATCCATATCTCCGTATGACCTAATGACATTAGTCGTCAGCAAAGGTTTGCCTATTAAGTAATCAACGGTATTGTTATTGGGGACATGCGTTTGGATCGCGTCACGACGCGTCTGAGTCACTGAGGTACCCTTAAGGAGTGTGTATCAGCGGCGAGCGTAAATATACACCTGCGCAGTATTGCCAATACCAGACTGTCTTTAAATTACATATCCGGGAAAAACGATCAGCTTTCACAAAATCCCCAGCACCCCCGAGGCCTAGAACCACCAAACCTGGTGTGTAACTAATATATTGATGTTCGAGCAGACACACAGTGATTCGGGGCCACGTTTTTCATGGGGTTAGGATCAGTTTCTGTGATGAATGTTTCTCCAGCTGTCACATCGTGGCTCATACAAGTTAAATGCATTCGTCGCCTGAAGGCGTTCTAACAGCAGCGATGGAAAGACGGATCAGGCATTTGCTTACCAGTTCTCTAATGATTTGGACTTGGGGGCTCGCCGACGATATGGACGGCATGTGGCGTGGACGGGTTAACACGTCATTAACATCCGCAGTGACGGATACGAGCACGACGCCATGTGACTAGGGCCCATAGTGGCCTCACGATAGCACAATTTCCTGCGCATTAAATGCCGAGAGGGTTATACCCCCTTTACTCTCTGCGTTGTCCATTGTGAATAGTGTCGAGCCATCGCGCACTTGCGCCCACGCCGCCTGTAGTGGCCATTCAGAAGGATTAGTTAATGACGGTCCCAAAAGTTGACATAGTCCTCGGACCGTCATAATGGATGTTTATATTTTTTATCGAATATGTAGGGTGAAAGGCGCCGGGCTCCGTACGCGGCGCCAATTGTACATCGGTCAAGGTGAAGGAACATACGTCACAACAGATGGTGTCTTTAGGTTATTGCAAAATCATCTAAGTCCGACCACACTTGCTGCACACGTTGTGCGCTGCTGTAACTTATAACGTGTATTTCGATAACCCTGAGTTCCCGTCCTTCGTTAGGGCAAGACCTTCGAGTCTGTGGGAATCAATTGAGTAAATGATAGACAACTATTTTTCTACCAACCCGTTGACTCCTTCACTAGAATCGTGAGTCCGAGTAATATGCTCCAGAGGATTCTCACTACACTGGGTAGTTCCCCATGTCAGGTTTCCTCCCCCATGAGTATGTACTCTCATAGAATGACAGAATCTGTCGGGCTAGTTGAGGGTCGGTATGTAGAAGTTGATAACCTCAACTACGTATGATTAATCGAGCCCCGATCTGGCGCTAAATTCTGGAGTGCTCGCCTGATGGGCTGCTTCTTGCATAAAAATACGTCCGAGATCAGGAAACCGTGTGGACCTAGGTGAGCAACGCCGGAGCACAGTAAATTCATACATCGTTCCGACCACGTTCTTCTGTTCCTGATCTAATCAAAACTGCCCGGGCAGGTGTAACCCCACAGTTCTCTGGCAGATCCTTGCAGATGCCACGTCCGTAGGGTCACTCCATTCCTCTAAAAATGGTCCATTGAGCCAACCCAGGCTAGTGGACTACTCTACAGAACAAATGTATTATGTCATGGTCTGCTTATGCTCCAACAAATCGTCACGTGATTGACCGCAGTCAGACTGCACATGACAGCTGGTCATCTGCGAGGTGTCCCTAAGGCTTCCTTCCCAGCAGAATTCAGATCGCGGTCTAACCACGTCTCCTAGGCGCGAGCATGCCTTGTAACGGACACGCAAAATTGCAATTAGCAAGTGAAAATAACAGCTAGATCGCCGATACTCACTCAGCTAGGTTCTGAGTCAGCACGAGTTCAGTCCATTAGAAACGGGGTCTTAGCTAAGCAGTCAAGCTGGCTCATGTCCAGGCAGGAACTACTGTATGGAGTACTCACCCACGTTCAACTTCCCCCTTTAAGCCATACACACTGCTCGTTTATATGGGGCGGAAGGTTTTTTAAAGCGGCACGCTCGTGGGCGAGCCCTCCAGCAGTCACTGGAAATTCCAGCACTCACGAAAGCGGCCAGGCCCCTAAGCTATGATCACACATTACCAAGCCTCCCAGTGAACGGACTCTCCGCATCTTGCGTCGAGGTAAGGCGACCTTGTCCTCGCGCATGCCTTTCGGTGACTCTAAAGAAACTGTTATAAGAGGGAGTAGTTTGTAGCACGATTGAAACGGGGGTGTTCCCGCGGATTCACTAGCCTCGCTGCTTTTAGCTGCATGGCATACTTAAAGACCGCATGGGACACTACTATGGACAGCCCGACAGCAACGGAAACGAAGACCATACACAAGAGGAATAAAGTAGGGTAGCCGATTGCGGTGGAATTGTTGCCGTTCTTATCGGGCGACAAAATTCGCCGCGCTCAGAGGTCCTGTCTTTTTCAGGACGCGGTCCTCATACTTCTTCCATTGTCCCGCAGGAGCATTGAGTTTGCCAGATCGGGCCGTGCTGGCTGTCTGTCCCCCGGAACACCACCGAAAGCCGAAACAGCTTGGTCCTCGATGCCCGGCCCCGCTGATTGATCTCACACTGTCTTCCACCGAGAGATGTACTGCTAAACTTGCCTTGAGTCATACAAGGCCACCCCGCTGGGTTAACGTGAACAACTAAATCTCACTAGTGGGGCCCCAGCTAAGCGCACAATCCGGGGTTTGACGGCTTGGGTGTGGATGATAAAGTTAGCGGATCTTTTGACCGTTAAATTTAACAAGGGAGGAGATTTCAAAGTTGCAGTTTCCTACTCCTATGTAACGGGATTCCCTAGTGCTCTGAGCCTAGGGGTACGGCCTCAATCATATTCGCGACAGGAGTGAGGCAATTCCAACGGCCAAGACGGGAACTCTCGTCACCGCACTTCCCTAACACTGATCGGTGTGGAGGCACGTGCTTCGACTCCGAGAGTGCATAAACCGGCTCCGGCATTCTATAACAGGGCAACTTCCCCGATACGCAGAAACCGAAGTCATTTGTTAACATTTCTATACGCAAGATCTGATCCGCGTTTTAGAATCACCCCGGCGTAGGGGGACTTACCGCTCTACCACTGCTTAGTGAGAGGGCCGGTGATTAGACTGTAGCGTTTCAGCGAGGCCGATCAGCAACCGTCTCTTTAGAGGGTGAAAGCGGTACCTCGATGTGGCGATAGTGAGGTGGAACGGCGCCTGTAGTATCCGTGGGTGGATGTCTTTCATTCTAGACTGGCACGGATTTAACATGTTGCGAGACTTAATCCCAGCCTCACTAAGTCACCTGGCGAGTATTTCGGGCGGATGCTTTACCCAGATAGTTAACAAAACGTCTTTAGTTCAAGAACCCAAAACGCAACCCGACCGGATAGGCCTCATCTCGTTCAGATATTGCATTAATTAAGCTCCGAGGTACACTGACGTCACGCGACTACTTCTGGGCTCGAGTAGCTCGTCAGATAGGGGAAAGTAAGCGGATTATTTGCATTCGCATCTACGCATTCAGATCGTTTGCTGAGAGCCGCCACGAGTCTGGATGTGTACGGCAATTGCATTACCACCTGTCGACAAACCCCTCGGCGCGGGACGTCCTATCAGACAAGGGGCACGGTATGCGACCACGCAACGCAGCTCACGGCAGAATAGTCCGGTTACATTAGCAATAATAGGGATAAACCGCTCACAATCAGCACCAAACCAATTCGTTAGTGCCCGGATAGCAAATAGCGGATCCATTACGTAGATACGTCCGCCATGTGCGCCTAGGCCGGAGAGCCCGGGCAATCCAACGTGAGTTGAGGTTAGTTGAGCTCAAGGACGAGCTAGGTGCAAGGCATATGTCAGTGGCATGGAGCGACGTCACCTCGGGCGATAGAATTCACTATTAGGTACGATTGTGACTAGCGGATATCGACAAACTCCGTTGCCACGGTGTAGAATCTGGAGATACTATGGTATAGGTTCGGAACCCTCCCCACGGTTTTTCCCTCTAGAGCTGCGGATATAAATGGTATTCTCTGTGGAGGCTAAACAGTCTGTGGGACCCTCTAGCAGAACGTTATGCATGGCAATTCTTCAGTCTTGGAACATCTGGGCTTCAGGATGAATTAGACGCCGAATCCAGCGCGTCGAGGATGTTGCTCAATTTTAGGTTCGCGCAGCCACCCCGCTCTGCCCGTGCAGTCCGCTTGTCAAAGTGTTCCGCGCTCTAACTCACACCCGTGTGTTAGATCGTACCCAATATTTCTAATCAGATTGTAGGTTCTTAAACCACAATCCTGACCGGGTCGCATTATGAGATAGCACGCTCGCAGAGCTGAGCTCTGACAAATCGGCGGATGTCATCCAGGGGGCACTGATTGACCAAGTCTTAGATTAGTTACCGTTTATAGGCGTAGAAAATTGCAAATGATATTATGGATATTTACCTGCTGATACAAGGAGGCACAGAACCGATCGTCATTCCCGGGAATCAATCGCATGTAATTGTCGACGGGTGCGCGTATTTCAGCCGGTAGTTGGTTAATAAATCCTACTATACCGCCTTACCAAGCAATCAAGCCGAAACGGGGATGTCCCACTATCGGCGCTGCCACGGACCTTCCGCTCTGTCGGCCGGAATACCAGGCCGGTTAGGGTATATAGGCTGCATTGTCCTTATAATATCGTAAGGGCGTAAAAAAAGCACCTCTGCCACCAGTACGCCCCTTTTTAGCATAAATTTCACTAGACGGGTCTATGTGAATCGCAAACCTAACCAGGCCCGGTCTACCTGGAAGCTACTGTTAGATTACGTGTAAAGCCTACCAGTAACCTGTGCGAAATATTGGACAGAAAAACAGGTCCTGGATAAGCGCCTTTATAGGGAAGTCTTGAGGCGCGGCATAAGTTCGTACGACACGGTCCGCAAACGAAGACAAACCGTTATAGCGCGGCAGGGATCCCGGGGTTCGTTCTGCTCAAGGTGCTGTATGGTTTAGCATAGCGAGTACCCAACCAACGGCGGCCCCTTGTCAACTTGGGACAATTCACAGCAGGGTGTCGCGGGATAGCAGCGTCACCATGTGTCTGAGAACTGCGGGGCCTATCAATGACTCCTGTGCAACTCTGAAGTTATCATGTCGTGGGGGCTGGAATAGCAGAGCAGCAGTTGGTTCACACGTAACATTGGCGGAGCTCCGGAGCTTGTGAGACTCCATCTGCGTGAGGTTGTGCGGGGGGGACCTCCATAAACTTGCATAGGCGCAGAACACCCAACAATGGTAGTTACAATTAGACCACATGACGCGCTCGTTCACAGATGGCATATCCCTCTTAGCTGTTCGCTTCGCGTCCTAAACCCCGCATAGAGACCGTAGAAACTCTCCGGAATTTACAATGTGTGGCCATGTGACGCGTATTCGTATTCCTGCAGCTGCGGGCCCGAATGGTGCGAATTAGTATATCTTGTAAGTGACCATGAGGGTGCAGCCATGACTTAC\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 随机选取特定数目的碱基\n",
    "bases = ['A', 'T', 'G', 'C']\n",
    "seqs = random.choices(bases, k=8192)\n",
    "# 生成输入的碱基序列\n",
    "text = ''.join(seqs)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccbf116",
   "metadata": {},
   "source": [
    "## 步骤4：序列编码\n",
    "\n",
    "使用分词器将DNA序列转换为模型可以处理的格式：\n",
    "\n",
    "- **tokenizer(text, return_tensors=\"pt\")**：将文本序列转换为PyTorch张量\n",
    "- **return_tensors=\"pt\"**：返回PyTorch格式的张量\n",
    "- **inputs**：包含编码后的序列信息，包括input_ids、attention_mask等\n",
    "\n",
    "这一步将原始的DNA字符串转换为数字化的token序列，每个碱基或碱基组合对应一个唯一的token ID。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cd59150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6, 5, 7,  ..., 7, 8, 5]])\n"
     ]
    }
   ],
   "source": [
    "# 编码文本\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 查看编码后的token序列\n",
    "print(inputs['input_ids'])\n",
    "\n",
    "# 数据加载到GPU\n",
    "inputs = {k: v.cuda() for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efca618",
   "metadata": {},
   "source": [
    "## 步骤5：模型推理\n",
    "\n",
    "通过预训练模型进行前向传播，获取embedding：\n",
    "\n",
    "- **torch.no_grad()**：禁用梯度计算，因为我们只需要推理，不需要训练\n",
    "- **model(\\*\\*inputs)**：将编码后的序列输入模型\n",
    "- **outputs**：模型输出，包含最后一层的logits和所有层的隐藏状态\n",
    "\n",
    "这一步是embedding提取的核心，模型会根据预训练的知识将序列转换为高维向量表示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5a61881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型推理\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b301e",
   "metadata": {},
   "source": [
    "## 步骤6：提取各层Embedding\n",
    "\n",
    "获取模型每一层的隐藏状态（embedding）：\n",
    "\n",
    "- **outputs.hidden_states**：包含所有层的隐藏状态，是一个元组\n",
    "- **hidden_states[i]**：第i层的embedding向量\n",
    "- **shape**：每个embedding的形状为[batch_size, sequence_length, hidden_size]\n",
    "\n",
    "**重要说明**：\n",
    "- 不同层的embedding捕获不同层次的语义信息\n",
    "- 浅层通常捕获局部特征（如碱基模式）\n",
    "- 深层捕获更抽象的语义信息（如功能域、结构特征）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c12ac8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 embedding (torch.Size([1, 8192, 1024])): tensor([-0.0052, -0.0056, -0.0047, -0.0013, -0.0018, -0.0060, -0.0099, -0.0002,\n",
      "        -0.0137, -0.0059], device='cuda:0', dtype=torch.bfloat16)\n",
      "--------------------------------------------------\n",
      "Layer 1 embedding (torch.Size([1, 8192, 1024])): tensor([-0.0801,  0.0388, -0.0127,  0.0366,  0.0718, -0.0270, -0.1182, -0.0239,\n",
      "        -0.0439,  0.0203], device='cuda:0', dtype=torch.bfloat16)\n",
      "--------------------------------------------------\n",
      "Layer 2 embedding (torch.Size([1, 8192, 1024])): tensor([-0.1338, -0.0796,  0.1348, -0.1982,  0.2695,  0.0879, -0.1201, -0.1416,\n",
      "         0.0037,  0.2324], device='cuda:0', dtype=torch.bfloat16)\n",
      "--------------------------------------------------\n",
      "Layer 3 embedding (torch.Size([1, 8192, 1024])): tensor([-0.2275, -0.1211,  0.2461, -0.2773,  0.3984,  0.0608, -0.3828, -0.0195,\n",
      "        -0.1387,  0.3086], device='cuda:0', dtype=torch.bfloat16)\n",
      "--------------------------------------------------\n",
      "Layer 4 embedding (torch.Size([1, 8192, 1024])): tensor([-2.9375, -0.5547, -0.7969, -1.7031,  2.9219,  0.5234, -1.4062,  0.6250,\n",
      "        -2.1406,  2.0625], device='cuda:0', dtype=torch.bfloat16)\n",
      "--------------------------------------------------\n",
      "Layer 5 embedding (torch.Size([1, 8192, 1024])): tensor([-2.9844, -0.5000, -0.8906, -1.7578,  2.9688,  0.6094, -1.4375,  0.6758,\n",
      "        -2.2188,  1.9688], device='cuda:0', dtype=torch.bfloat16)\n",
      "--------------------------------------------------\n",
      "Layer 6 embedding (torch.Size([1, 8192, 1024])): tensor([-2.8906, -0.3652, -0.9453, -1.7578,  3.0938,  0.6523, -1.4297,  0.7266,\n",
      "        -2.0938,  1.8906], device='cuda:0', dtype=torch.bfloat16)\n",
      "--------------------------------------------------\n",
      "Layer 7 embedding (torch.Size([1, 8192, 1024])): tensor([-2.9844, -0.2578, -0.9570, -1.6875,  3.2344,  0.8867, -1.3594,  0.8164,\n",
      "        -2.0625,  1.8594], device='cuda:0', dtype=torch.bfloat16)\n",
      "--------------------------------------------------\n",
      "Layer 8 embedding (torch.Size([1, 8192, 1024])): tensor([-3.1250, -0.1621, -0.9961, -1.6641,  3.3125,  1.0469, -1.4297,  1.0391,\n",
      "        -1.9219,  1.6094], device='cuda:0', dtype=torch.bfloat16)\n",
      "--------------------------------------------------\n",
      "Layer 9 embedding (torch.Size([1, 8192, 1024])): tensor([-3.1094, -0.4355, -1.1328, -1.6953,  3.4531,  1.3281, -1.5781,  1.0781,\n",
      "        -1.8203,  1.7500], device='cuda:0', dtype=torch.bfloat16)\n",
      "--------------------------------------------------\n",
      "Layer 10 embedding (torch.Size([1, 8192, 1024])): tensor([-3.1094, -0.6484, -1.2031, -2.1250,  3.6875,  1.3750, -1.1172,  0.8906,\n",
      "        -1.6719,  1.6250], device='cuda:0', dtype=torch.bfloat16)\n",
      "--------------------------------------------------\n",
      "Layer 11 embedding (torch.Size([1, 8192, 1024])): tensor([-2.6875, -0.8125, -1.2188, -1.1797,  3.9062,  1.1875, -1.4766,  1.9219,\n",
      "        -1.6250,  1.3984], device='cuda:0', dtype=torch.bfloat16)\n",
      "--------------------------------------------------\n",
      "Layer 12 embedding (torch.Size([1, 8192, 1024])): tensor([-1.5703, -0.5430, -0.6875, -0.6367,  2.2812,  0.6953, -0.6836,  1.1406,\n",
      "        -0.7734,  0.4102], device='cuda:0', dtype=torch.bfloat16)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 获取所有层的隐藏状态\n",
    "hidden_states = outputs.hidden_states  # 元组，包含每一层的隐藏状态\n",
    "\n",
    "# 遍历每一层的embedding并显示关键信息\n",
    "for i, layer_embedding in enumerate(hidden_states):\n",
    "    print(f\"Layer {i} embedding ({layer_embedding.shape}): {layer_embedding[0, 0, :10]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7276cd1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b3aed41",
   "metadata": {},
   "source": [
    "---\n",
    "# 使用Genos的包直接获取embedding\n",
    "\n",
    "\n",
    "注意： 因为当前资源有限，目前API提供的的模型支持1.2B和10B，最大embedding的长度为**128k**，并且只返回最后一层的embedding\n",
    "\n",
    "- 参数说明\n",
    "    - sequence，序列，最大长度不超过128k\n",
    "    - model_name, 模型名称，“Genos-1.2B”或者“Genos-10B”\n",
    "    - pooling_method，池化方法，“mean”：平均池化，“max”：最大池化，“min”：最小池化，“last”：取最后一个token的embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b087cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0116, -0.0371, -0.0466,  ..., -0.2812, -0.1299,  0.0261]])\n"
     ]
    }
   ],
   "source": [
    "from genos import create_client\n",
    "\n",
    "client = create_client(token=\"<your_api_key>\")\n",
    "\n",
    "result = client.get_embedding(sequence=text, model_name=\"Genos-1.2B\", pooling_method=\"mean\")\n",
    "print(result['result']['embedding'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd1aa3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a1431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81ec539d",
   "metadata": {},
   "source": [
    "### Embedding的应用场景\n",
    "\n",
    "提取的embedding可以用于：\n",
    "\n",
    "1. **序列相似性计算**：\n",
    "   ```python\n",
    "   # 计算两个序列的余弦相似度\n",
    "   similarity = cosine_similarity(embedding1, embedding2)\n",
    "   ```\n",
    "\n",
    "2. **序列分类**：\n",
    "   ```python\n",
    "   # 使用embedding训练分类器\n",
    "   classifier = train_classifier(embeddings, labels)\n",
    "   ```\n",
    "\n",
    "3. **聚类分析**：\n",
    "   ```python\n",
    "   # 对序列进行聚类\n",
    "   clusters = kmeans_clustering(embeddings)\n",
    "   ```\n",
    "\n",
    "4. **降维可视化**：\n",
    "   ```python\n",
    "   # 使用t-SNE或PCA进行降维可视化\n",
    "   reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136ae7ae",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "通过本教程，我们学习了：\n",
    "\n",
    "### 核心概念\n",
    "- **Embedding**：将生物序列转换为数值向量的技术\n",
    "- **预训练模型**：专门为生物序列训练的大型语言模型\n",
    "- **分层特征**：不同层捕获不同层次的序列信息\n",
    "\n",
    "### 技术流程\n",
    "1. 加载预训练的生物序列模型\n",
    "2. 将DNA序列编码为token\n",
    "3. 通过模型推理获取embedding\n",
    "4. 分析各层的特征表示\n",
    "\n",
    "### 实际价值\n",
    "- **加速研究**：快速分析大量生物序列\n",
    "- **提高准确性**：利用预训练知识提升预测性能\n",
    "- **支持下游任务**：为分类、聚类、相似性分析等提供基础\n",
    "\n",
    "### 下一示例\n",
    "1. 利用提取的embedding进行下游的人种预测任务\n",
    "2. 利用提取的embedding进行下游的变异预测任务\n",
    "3. RNA seq预测\n",
    "\n",
    "\n",
    "**恭喜！** 你已经掌握了生物序列embedding提取的基本方法！\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hgtest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
